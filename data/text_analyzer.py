"""
í…ìŠ¤íŠ¸ ë° í‚¤ì›Œë“œ ë¶„ì„ ì „ìš© ëª¨ë“ˆ
í‚¤ì›Œë“œ ì¶”ì¶œ, í…ìŠ¤íŠ¸ ì •ë¦¬, ì–¸ì–´ ì²˜ë¦¬ ë‹´ë‹¹
"""

import re
from collections import Counter

# ì„ íƒì  import
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì œí•œë©ë‹ˆë‹¤.")

class TextAnalyzer:
    """í…ìŠ¤íŠ¸ ë° í‚¤ì›Œë“œ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, language="ko"):
        """
        í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            language (str): ë¶„ì„ ì–¸ì–´ ("ko" ë˜ëŠ” "en")
        """
        self.language = language
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        if KONLPY_AVAILABLE and language == "ko":
            try:
                self.okt = Okt()
                print("âœ… í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.okt = None
        else:
            self.okt = None
        
        # ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì„¤ì •
        self.stopwords = self._load_stopwords(language)
    
    def extract_keywords_from_title(self, title, max_keywords=10):
        """
        ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            title (str): ì˜ìƒ ì œëª©
            max_keywords (int): ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: í‚¤ì›Œë“œ ëª©ë¡
        """
        if not title:
            return []
        
        if self.language == "ko" and self.okt:
            return self._extract_korean_keywords(title, max_keywords)
        else:
            return self._extract_english_keywords(title, max_keywords)
    
    def _extract_korean_keywords(self, text, max_keywords):
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ
            morphs = self.okt.pos(text, stem=True)
            keywords = [word for word, pos in morphs 
                       if pos in ['Noun', 'Adjective'] and len(word) > 1]
            
            # ë¶ˆìš©ì–´ ì œê±°
            keywords = [word for word in keywords if word not in self.stopwords['ko']]
            
            # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            keyword_counts = Counter(keywords)
            return [keyword for keyword, _ in keyword_counts.most_common(max_keywords)]
            
        except Exception as e:
            print(f"í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._extract_english_keywords(text, max_keywords)
    
    def _extract_english_keywords(self, text, max_keywords):
        """ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
            cleaned_text = re.sub(r'[^a-zA-Zê°€-í£\s]', ' ', text)
            words = cleaned_text.lower().split()
            
            # ë¶ˆìš©ì–´ ì œê±°
            keywords = [word for word in words 
                       if len(word) > 2 and word not in self.stopwords['en']]
            
            # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            keyword_counts = Counter(keywords)
            return [keyword for keyword, _ in keyword_counts.most_common(max_keywords)]
            
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_trending_keywords(self, all_titles, max_keywords=20):
        """
        ì „ì²´ ì œëª©ì—ì„œ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            all_titles (list): ëª¨ë“  ì˜ìƒ ì œëª© ëª©ë¡
            max_keywords (int): ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: íŠ¸ë Œë”© í‚¤ì›Œë“œ ëª©ë¡ (ë¹ˆë„ìˆœ)
        """
        all_keywords = []
        
        for title in all_titles:
            keywords = self.extract_keywords_from_title(title, max_keywords=50)
            all_keywords.extend(keywords)
        
        keyword_counts = Counter(all_keywords)
        return [keyword for keyword, count in keyword_counts.most_common(max_keywords)]
    
    def analyze_keyword_frequency(self, titles):
        """
        í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        
        Args:
            titles (list): ì œëª© ëª©ë¡
            
        Returns:
            dict: í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ê²°ê³¼
        """
        all_keywords = []
        title_keyword_map = {}
        
        # ê° ì œëª©ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
        for i, title in enumerate(titles):
            keywords = self.extract_keywords_from_title(title)
            all_keywords.extend(keywords)
            title_keyword_map[i] = keywords
        
        # ë¹ˆë„ ë¶„ì„
        keyword_counts = Counter(all_keywords)
        total_keywords = len(all_keywords)
        unique_keywords = len(keyword_counts)
        
        # í‚¤ì›Œë“œë³„ ìƒì„¸ ì •ë³´
        keyword_details = {}
        for keyword, count in keyword_counts.items():
            # ì´ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì œëª©ë“¤
            containing_titles = []
            for title_idx, keywords in title_keyword_map.items():
                if keyword in keywords:
                    containing_titles.append({
                        'index': title_idx,
                        'title': titles[title_idx] if title_idx < len(titles) else ''
                    })
            
            keyword_details[keyword] = {
                'count': count,
                'frequency': round(count / total_keywords * 100, 2),
                'title_coverage': round(len(containing_titles) / len(titles) * 100, 2),
                'containing_titles': containing_titles[:5]  # ìƒìœ„ 5ê°œë§Œ
            }
        
        return {
            'total_keywords': total_keywords,
            'unique_keywords': unique_keywords,
            'top_keywords': [
                {
                    'keyword': keyword,
                    **details
                }
                for keyword, details in sorted(
                    keyword_details.items(),
                    key=lambda x: x[1]['count'],
                    reverse=True
                )[:20]
            ],
            'keyword_diversity': round(unique_keywords / total_keywords * 100, 2) if total_keywords > 0 else 0
        }
    
    def clean_text(self, text):
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # 1. HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£.,!?]', ' ', text)
        
        # 3. ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # 4. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def extract_hashtags(self, text):
        """
        í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        
        Args:
            text (str): í…ìŠ¤íŠ¸
            
        Returns:
            list: í•´ì‹œíƒœê·¸ ëª©ë¡
        """
        if not text:
            return []
        
        # #í‚¤ì›Œë“œ í˜•íƒœì˜ í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = re.findall(r'#(\w+)', text)
        
        # ì •ë¦¬ ë° í•„í„°ë§
        cleaned_hashtags = []
        for tag in hashtags:
            cleaned_tag = self.clean_text(tag)
            if len(cleaned_tag) >= 2:
                cleaned_hashtags.append(cleaned_tag)
        
        return cleaned_hashtags
    
    def analyze_text_patterns(self, texts):
        """
        í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
        
        Args:
            texts (list): í…ìŠ¤íŠ¸ ëª©ë¡
            
        Returns:
            dict: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        if not texts:
            return {}
        
        # ê¸°ë³¸ í†µê³„
        lengths = [len(text) for text in texts if text]
        word_counts = [len(text.split()) for text in texts if text]
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ê°ì§€
        patterns = {
            'with_numbers': len([t for t in texts if re.search(r'\d', t)]),
            'with_exclamation': len([t for t in texts if '!' in t]),
            'with_question': len([t for t in texts if '?' in t]),
            'with_emoji': len([t for t in texts if re.search(r'[ğŸ˜€-ğŸ™]', t)]),
            'with_brackets': len([t for t in texts if re.search(r'[\[\](){}]', t)]),
            'with_quotes': len([t for t in texts if re.search(r'["\'\`]', t)])
        }
        
        # ì–¸ì–´ ê°ì§€
        korean_count = len([t for t in texts if re.search(r'[ê°€-í£]', t)])
        english_count = len([t for t in texts if re.search(r'[a-zA-Z]', t)])
        
        return {
            'total_texts': len(texts),
            'length_stats': {
                'min': min(lengths) if lengths else 0,
                'max': max(lengths) if lengths else 0,
                'avg': round(sum(lengths) / len(lengths), 2) if lengths else 0
            },
            'word_count_stats': {
                'min': min(word_counts) if word_counts else 0,
                'max': max(word_counts) if word_counts else 0,
                'avg': round(sum(word_counts) / len(word_counts), 2) if word_counts else 0
            },
            'pattern_analysis': {
                **patterns,
                'pattern_percentages': {
                    key: round(count / len(texts) * 100, 2)
                    for key, count in patterns.items()
                }
            },
            'language_distribution': {
                'korean': korean_count,
                'english': english_count,
                'korean_percentage': round(korean_count / len(texts) * 100, 2),
                'english_percentage': round(english_count / len(texts) * 100, 2)
            }
        }
    
    def find_similar_texts(self, target_text, text_list, threshold=0.7):
        """
        ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ì°¾ê¸°
        
        Args:
            target_text (str): ê¸°ì¤€ í…ìŠ¤íŠ¸
            text_list (list): ë¹„êµí•  í…ìŠ¤íŠ¸ ëª©ë¡
            threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            list: ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë“¤ê³¼ ìœ ì‚¬ë„ ì ìˆ˜
        """
        similar_texts = []
        
        target_keywords = set(self.extract_keywords_from_title(target_text))
        
        for i, text in enumerate(text_list):
            if text == target_text:
                continue
            
            text_keywords = set(self.extract_keywords_from_title(text))
            
            # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
            intersection = len(target_keywords & text_keywords)
            union = len(target_keywords | text_keywords)
            
            similarity = intersection / union if union > 0 else 0
            
            if similarity >= threshold:
                similar_texts.append({
                    'index': i,
                    'text': text,
                    'similarity': round(similarity, 3),
                    'common_keywords': list(target_keywords & text_keywords)
                })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_texts.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similar_texts
    
    def _load_stopwords(self, language):
        """ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ë¡œë“œ"""
        stopwords = {
            'ko': {
                'ê²ƒ', 'ìˆ˜', 'ë‚´', 'ê±°', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë”°ë¼', 'ëŒ€í•´', 
                'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 
                'ì €ê¸°', 'ì§€ê¸ˆ', 'ê·¸ë•Œ', 'ì´ë•Œ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ë…„', 
                'ì›”', 'ì¼', 'ì‹œê°„', 'ë¶„', 'ì´ˆ', 'ë•Œ', 'ë™ì•ˆ', 'ì‚¬ì´', 'ë‹¤ìŒ',
                'ì´ì „', 'ì „ì²´', 'ë¶€ë¶„', 'ëª¨ë“ ', 'ê°ê°', 'í•˜ë‚˜', 'ë‘˜', 'ì…‹'
            },
            'en': {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 
                'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 
                'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 
                'will', 'would', 'could', 'should', 'may', 'might', 'must', 
                'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 
                'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
                'my', 'your', 'his', 'her', 'its', 'our', 'their'
            }
        }
        
        return stopwords
    
    def get_language_support_info(self):
        """ì–¸ì–´ ì§€ì› ì •ë³´ ë°˜í™˜"""
        return {
            'current_language': self.language,
            'konlpy_available': KONLPY_AVAILABLE,
            'morphological_analysis': self.okt is not None,
            'supported_languages': ['ko', 'en'],
            'features': {
                'keyword_extraction': True,
                'text_cleaning': True,
                'hashtag_extraction': True,
                'pattern_analysis': True,
                'similarity_analysis': True,
                'korean_morphology': self.okt is not None
            }
        }


class KeywordTrendAnalyzer:
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ì „ë¬¸ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, text_analyzer):
        """
        í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            text_analyzer: TextAnalyzer ì¸ìŠ¤í„´ìŠ¤
        """
        self.text_analyzer = text_analyzer
    
    def analyze_keyword_trends_over_time(self, time_series_data):
        """
        ì‹œê°„ì— ë”°ë¥¸ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
        
        Args:
            time_series_data (list): [{'date': '2024-01-01', 'texts': [...]}] í˜•íƒœ
            
        Returns:
            dict: ì‹œê°„ë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ
        """
        keyword_timeline = {}
        
        for data_point in time_series_data:
            date = data_point['date']
            texts = data_point['texts']
            
            # í•´ë‹¹ ë‚ ì§œì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for text in texts:
                keywords = self.text_analyzer.extract_keywords_from_title(text)
                all_keywords.extend(keywords)
            
            keyword_counts = Counter(all_keywords)
            keyword_timeline[date] = dict(keyword_counts)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trending_keywords = self._calculate_keyword_velocity(keyword_timeline)
        
        return {
            'timeline': keyword_timeline,
            'trending_keywords': trending_keywords,
            'analysis_period': {
                'start_date': min(keyword_timeline.keys()) if keyword_timeline else None,
                'end_date': max(keyword_timeline.keys()) if keyword_timeline else None,
                'total_days': len(keyword_timeline)
            }
        }
    
    def _calculate_keyword_velocity(self, timeline):
        """í‚¤ì›Œë“œ ìƒìŠ¹/í•˜ë½ ì†ë„ ê³„ì‚°"""
        if len(timeline) < 2:
            return []
        
        dates = sorted(timeline.keys())
        velocities = {}
        
        for keyword in set().union(*timeline.values()):
            counts = []
            for date in dates:
                count = timeline[date].get(keyword, 0)
                counts.append(count)
            
            # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
            if len(counts) >= 2:
                velocity = counts[-1] - counts[0]  # ë§ˆì§€ë§‰ - ì²«ë²ˆì§¸
                avg_count = sum(counts) / len(counts)
                
                if avg_count > 0:
                    velocities[keyword] = {
                        'velocity': velocity,
                        'relative_velocity': velocity / avg_count,
                        'avg_frequency': avg_count,
                        'trend': 'rising' if velocity > 0 else 'falling' if velocity < 0 else 'stable'
                    }
        
        # ì†ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_velocities = sorted(
            velocities.items(),
            key=lambda x: x[1]['relative_velocity'],
            reverse=True
        )
        
        return [
            {
                'keyword': keyword,
                **data
            }
            for keyword, data in sorted_velocities[:20]
        ]
    
    def find_keyword_clusters(self, texts, min_cluster_size=3):
        """
        í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            texts (list): í…ìŠ¤íŠ¸ ëª©ë¡
            min_cluster_size (int): ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
            
        Returns:
            dict: í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„° ì •ë³´
        """
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        text_keywords = []
        for text in texts:
            keywords = self.text_analyzer.extract_keywords_from_title(text)
            text_keywords.append(set(keywords))
        
        # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
        all_keywords = set().union(*text_keywords)
        cooccurrence = {}
        
        for kw1 in all_keywords:
            cooccurrence[kw1] = {}
            for kw2 in all_keywords:
                if kw1 != kw2:
                    count = sum(1 for keywords in text_keywords 
                              if kw1 in keywords and kw2 in keywords)
                    cooccurrence[kw1][kw2] = count
        
        # ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ (ë†’ì€ ë™ì‹œ ì¶œí˜„ ë¹ˆë„ ê¸°ì¤€)
        clusters = []
        used_keywords = set()
        
        for keyword in sorted(all_keywords, 
                            key=lambda k: sum(cooccurrence[k].values()), 
                            reverse=True):
            
            if keyword in used_keywords:
                continue
            
            # ì´ í‚¤ì›Œë“œì™€ ìì£¼ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë“¤ ì°¾ê¸°
            related = []
            for other_kw, count in cooccurrence[keyword].items():
                if count >= min_cluster_size and other_kw not in used_keywords:
                    related.append((other_kw, count))
            
            if len(related) >= min_cluster_size - 1:
                cluster_keywords = [keyword] + [kw for kw, _ in related[:5]]
                clusters.append({
                    'cluster_id': len(clusters) + 1,
                    'main_keyword': keyword,
                    'related_keywords': cluster_keywords[1:],
                    'size': len(cluster_keywords),
                    'strength': sum(count for _, count in related[:5])
                })
                
                used_keywords.update(cluster_keywords)
        
        return {
            'clusters': clusters,
            'total_clusters': len(clusters),
            'clustered_keywords': len(used_keywords),
            'unclustered_keywords': len(all_keywords) - len(used_keywords)
        }