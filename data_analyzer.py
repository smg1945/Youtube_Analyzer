"""
ë°ì´í„° ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤ (í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì • ë¶„ì„ ë“±)
"""

import re
from collections import Counter
from datetime import datetime
import config

# ì–¸ì–´ë³„ ë¶„ì„ ë„êµ¬
try:
    from konlpy.tag import Okt  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False
    print("KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from textblob import TextBlob  # ì˜ì–´ ê°ì • ë¶„ì„
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    print("TextBlobì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°ì • ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.")

class DataAnalyzer:
    def __init__(self, language="ko"):
        """
        ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            language (str): ë¶„ì„ ì–¸ì–´ ("ko" ë˜ëŠ” "en")
        """
        self.language = language
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        if KONLPY_AVAILABLE and language == "ko":
            self.okt = Okt()
        else:
            self.okt = None
    
    def extract_keywords_from_title(self, title, max_keywords=10):
        """
        ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            title (str): ì˜ìƒ ì œëª©
            max_keywords (int): ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: í‚¤ì›Œë“œ ëª©ë¡
        """
        if not title:
            return []
        
        if self.language == "ko" and self.okt:
            return self._extract_korean_keywords(title, max_keywords)
        else:
            return self._extract_english_keywords(title, max_keywords)
    
    def _extract_korean_keywords(self, text, max_keywords):
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ
            morphs = self.okt.pos(text, stem=True)
            keywords = [word for word, pos in morphs 
                       if pos in ['Noun', 'Adjective'] and len(word) > 1]
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'ê²ƒ', 'ìˆ˜', 'ë‚´', 'ê±°', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë”°ë¼', 'ëŒ€í•´', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ'}
            keywords = [word for word in keywords if word not in stopwords]
            
            # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            keyword_counts = Counter(keywords)
            return [keyword for keyword, _ in keyword_counts.most_common(max_keywords)]
            
        except Exception as e:
            print(f"í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._extract_english_keywords(text, max_keywords)
    
    def _extract_english_keywords(self, text, max_keywords):
        """ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
            cleaned_text = re.sub(r'[^a-zA-Zê°€-í£\s]', ' ', text)
            words = cleaned_text.lower().split()
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                        'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were',
                        'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë‚´', 'ê±°', 'ë•Œë¬¸', 'ìœ„í•´'}
            
            keywords = [word for word in words 
                       if len(word) > 2 and word not in stopwords]
            
            # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            keyword_counts = Counter(keywords)
            return [keyword for keyword, _ in keyword_counts.most_common(max_keywords)]
            
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_comments_sentiment(self, comments):
        """
        ëŒ“ê¸€ ê°ì • ë¶„ì„
        
        Args:
            comments (list): ëŒ“ê¸€ ëª©ë¡
            
        Returns:
            dict: ê°ì • ë¶„ì„ ê²°ê³¼ {'positive': float, 'neutral': float, 'negative': float}
        """
        if not comments:
            return {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0}
        
        if TEXTBLOB_AVAILABLE:
            return self._analyze_sentiment_textblob(comments)
        else:
            return self._analyze_sentiment_keywords(comments)
    
    def _analyze_sentiment_textblob(self, comments):
        """TextBlobì„ ì‚¬ìš©í•œ ê°ì • ë¶„ì„"""
        positive_count = 0
        negative_count = 0
        neutral_count = 0
        
        for comment in comments:
            try:
                blob = TextBlob(comment['text'])
                polarity = blob.sentiment.polarity
                
                if polarity > 0.1:
                    positive_count += 1
                elif polarity < -0.1:
                    negative_count += 1
                else:
                    neutral_count += 1
                    
            except Exception as e:
                neutral_count += 1  # ë¶„ì„ ì‹¤íŒ¨ì‹œ ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        
        total = len(comments)
        if total == 0:
            return {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0}
            
        return {
            'positive': round(positive_count / total * 100, 2),
            'neutral': round(neutral_count / total * 100, 2),
            'negative': round(negative_count / total * 100, 2)
        }
    
    def _analyze_sentiment_keywords(self, comments):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        positive_keywords = {
            'ko': ['ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ë©‹ì§€ë‹¤', 'í›Œë¥­', 'ì™„ë²½', 'ì‚¬ë‘', 'ê°ì‚¬', 'ì¬ë°Œ', 'ì›ƒê²¨', 'êµ¿'],
            'en': ['good', 'great', 'amazing', 'awesome', 'perfect', 'love', 'best', 'wonderful', 'excellent']
        }
        
        negative_keywords = {
            'ko': ['ì‹«ë‹¤', 'ë³„ë¡œ', 'ìµœì•…', 'ë‚˜ì˜ë‹¤', 'ì§œì¦', 'í™”ë‚˜', 'ì‹¤ë§', 'ì•ˆì¢‹', 'ëª»í•´'],
            'en': ['bad', 'terrible', 'awful', 'hate', 'worst', 'disappointing', 'boring', 'stupid']
        }
        
        pos_words = positive_keywords.get(self.language, positive_keywords['en'])
        neg_words = negative_keywords.get(self.language, negative_keywords['en'])
        
        positive_count = 0
        negative_count = 0
        
        for comment in comments:
            text = comment['text'].lower()
            
            # ê¸ì • í‚¤ì›Œë“œ ì²´í¬
            if any(word in text for word in pos_words):
                positive_count += 1
            # ë¶€ì • í‚¤ì›Œë“œ ì²´í¬
            elif any(word in text for word in neg_words):
                negative_count += 1
        
        total = len(comments)
        neutral_count = total - positive_count - negative_count
        
        if total == 0:
            return {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0}
        
        return {
            'positive': round(positive_count / total * 100, 2),
            'neutral': round(neutral_count / total * 100, 2),
            'negative': round(negative_count / total * 100, 2)
        }
    
    def calculate_engagement_score(self, video_data):
        """
        ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            video_data (dict): ì˜ìƒ ë°ì´í„°
            
        Returns:
            float: ì°¸ì—¬ë„ ì ìˆ˜ (0-100)
        """
        try:
            stats = video_data.get('statistics', {})
            
            view_count = int(stats.get('viewCount', 0))
            like_count = int(stats.get('likeCount', 0))
            comment_count = int(stats.get('commentCount', 0))
            
            if view_count == 0:
                return 0.0
            
            # ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            like_ratio = (like_count / view_count) * 100
            comment_ratio = (comment_count / view_count) * 100
            
            # ê°€ì¤‘ì¹˜: ì¢‹ì•„ìš” 70%, ëŒ“ê¸€ 30%
            engagement_score = (like_ratio * 0.7 + comment_ratio * 0.3) * 1000
            
            # 0-100 ë²”ìœ„ë¡œ ì •ê·œí™”
            return min(round(engagement_score, 2), 100.0)
            
        except Exception as e:
            print(f"ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def format_duration(self, duration_seconds):
        """
        ì´ˆ ë‹¨ìœ„ durationì„ ì‹œ:ë¶„:ì´ˆ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            duration_seconds (int): ì´ˆ ë‹¨ìœ„ duration
            
        Returns:
            str: "HH:MM:SS" ë˜ëŠ” "MM:SS" í˜•íƒœ
        """
        if duration_seconds < 3600:  # 1ì‹œê°„ ë¯¸ë§Œ
            minutes = duration_seconds // 60
            seconds = duration_seconds % 60
            return f"{minutes:02d}:{seconds:02d}"
        else:  # 1ì‹œê°„ ì´ìƒ
            hours = duration_seconds // 3600
            minutes = (duration_seconds % 3600) // 60
            seconds = duration_seconds % 60
            return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    
    def determine_video_type(self, duration_seconds):
        """
        ì˜ìƒ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ìƒ ìœ í˜• ê²°ì •
        
        Args:
            duration_seconds (int): ì˜ìƒ ê¸¸ì´ (ì´ˆ)
            
        Returns:
            str: "ì‡¼ì¸ " ë˜ëŠ” "ë¡±í¼"
        """
        if duration_seconds <= config.SHORT_VIDEO_MAX_DURATION:
            return "ì‡¼ì¸ "
        else:
            return "ë¡±í¼"
    
    def calculate_views_per_day(self, video_data):
        """
        ì¼ì¼ í‰ê·  ì¡°íšŒìˆ˜ ê³„ì‚°
        
        Args:
            video_data (dict): ì˜ìƒ ë°ì´í„°
            
        Returns:
            float: ì¼ì¼ í‰ê·  ì¡°íšŒìˆ˜
        """
        try:
            published_at = video_data['snippet']['publishedAt']
            view_count = int(video_data['statistics'].get('viewCount', 0))
            
            # ì—…ë¡œë“œ ë‚ ì§œ íŒŒì‹±
            upload_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            current_date = datetime.now(upload_date.tzinfo)
            
            # ê²½ê³¼ ì¼ìˆ˜ ê³„ì‚°
            days_elapsed = (current_date - upload_date).days
            if days_elapsed == 0:
                days_elapsed = 1  # ìµœì†Œ 1ì¼ë¡œ ì²˜ë¦¬
            
            return round(view_count / days_elapsed, 2)
            
        except Exception as e:
            print(f"ì¼ì¼ í‰ê·  ì¡°íšŒìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def extract_trending_keywords(self, all_titles, max_keywords=20):
        """
        ì „ì²´ ì œëª©ì—ì„œ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            all_titles (list): ëª¨ë“  ì˜ìƒ ì œëª© ëª©ë¡
            max_keywords (int): ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: íŠ¸ë Œë”© í‚¤ì›Œë“œ ëª©ë¡ (ë¹ˆë„ìˆœ)
        """
        all_keywords = []
        
        for title in all_titles:
            keywords = self.extract_keywords_from_title(title, max_keywords=50)
            all_keywords.extend(keywords)
        
        keyword_counts = Counter(all_keywords)
        return [keyword for keyword, count in keyword_counts.most_common(max_keywords)]
    
    def calculate_outlier_score(self, current_video_stats, channel_avg_stats):
        """
        vidIQì˜ Outlier Scoreì™€ ìœ ì‚¬í•œ ì§€í‘œ ê³„ì‚°
        
        Args:
            current_video_stats (dict): í˜„ì¬ ì˜ìƒ í†µê³„
            channel_avg_stats (dict): ì±„ë„ í‰ê·  í†µê³„
            
        Returns:
            float: outlier score (1.0 = í‰ê· , 2.0 = í‰ê· ì˜ 2ë°°, ë“±)
        """
        if not channel_avg_stats or not current_video_stats:
            return 1.0
        
        try:
            current_views = int(current_video_stats.get('viewCount', 0))
            current_likes = int(current_video_stats.get('likeCount', 0))
            current_comments = int(current_video_stats.get('commentCount', 0))
            
            avg_views = channel_avg_stats.get('avg_views', 1)
            avg_likes = channel_avg_stats.get('avg_likes', 1)
            avg_comments = channel_avg_stats.get('avg_comments', 1)
            
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            if avg_views == 0:
                avg_views = 1
            if avg_likes == 0:
                avg_likes = 1
            if avg_comments == 0:
                avg_comments = 1
            
            # ê° ì§€í‘œë³„ ë°°ìˆ˜ ê³„ì‚°
            views_ratio = current_views / avg_views
            likes_ratio = current_likes / avg_likes
            comments_ratio = current_comments / avg_comments
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ outlier score ê³„ì‚°
            # ì¡°íšŒìˆ˜ 50%, ì¢‹ì•„ìš” 30%, ëŒ“ê¸€ 20% ê°€ì¤‘ì¹˜
            outlier_score = (views_ratio * 0.5 + likes_ratio * 0.3 + comments_ratio * 0.2)
            
            return round(outlier_score, 2)
            
        except Exception as e:
            print(f"Outlier score ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 1.0
    
    def categorize_outlier_score(self, outlier_score):
        """
        Outlier Scoreë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
        
        Args:
            outlier_score (float): outlier score
            
        Returns:
            str: ì¹´í…Œê³ ë¦¬ ("ğŸ”¥ ë°”ì´ëŸ´", "â­ íˆíŠ¸", "ğŸ“ˆ ì–‘í˜¸", "ğŸ˜ í‰ê· ", "ğŸ“‰ ì €ì¡°")
        """
        if outlier_score >= 5.0:
            return "ğŸ”¥ ë°”ì´ëŸ´"
        elif outlier_score >= 3.0:
            return "â­ íˆíŠ¸"
        elif outlier_score >= 1.5:
            return "ğŸ“ˆ ì–‘í˜¸"
        elif outlier_score >= 0.7:
            return "ğŸ˜ í‰ê· "
        else:
            return "ğŸ“‰ ì €ì¡°"