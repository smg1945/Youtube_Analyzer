"""
YouTube íŠ¸ë Œë“œ ë¶„ì„ ì „ìš© ëª¨ë“ˆ
í‚¤ì›Œë“œ íŠ¸ë Œë“œ, ê¸‰ìƒìŠ¹ ë¶„ì„, ì—°ê´€ì„± ë¶„ì„ ë‹´ë‹¹
"""

import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta

# ì„ íƒì  import (ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ëŒ€ì²´)
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False

class TrendAnalyzer:
    """YouTube íŠ¸ë Œë“œ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, youtube_client, language="ko"):
        """
        íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            youtube_client: YouTubeClient ì¸ìŠ¤í„´ìŠ¤
            language (str): ë¶„ì„ ì–¸ì–´ ("ko" ë˜ëŠ” "en")
        """
        self.client = youtube_client
        self.language = language
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        if KONLPY_AVAILABLE and language == "ko":
            self.okt = Okt()
        else:
            self.okt = None
    
    def analyze_trending_keywords(self, region_code='KR', max_results=200):
        """
        íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„
        
        Args:
            region_code (str): ì§€ì—­ ì½”ë“œ
            max_results (int): ë¶„ì„í•  ì˜ìƒ ìˆ˜
            
        Returns:
            dict: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        """
        print(f"ğŸ” íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ ({region_code})")
        
        try:
            # 1. íŠ¸ë Œë”© ì˜ìƒ ìˆ˜ì§‘
            from .video_search import TrendingVideoSearcher
            searcher = TrendingVideoSearcher(self.client)
            videos = searcher.get_category_trending_videos(region_code, max_results)
            
            if not videos:
                return {'error': 'íŠ¸ë Œë”© ì˜ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
            print(f"âœ… {len(videos)}ê°œ íŠ¸ë Œë”© ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
            keyword_stats = self._extract_keywords_from_videos(videos)
            
            # 3. íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°
            trend_results = self._calculate_trend_scores(keyword_stats)
            
            # 4. ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„
            related_keywords = self._analyze_keyword_relationships(keyword_stats)
            
            # 5. ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„
            temporal_trends = self._analyze_temporal_trends(videos, keyword_stats)
            
            return {
                'analysis_time': datetime.now().isoformat(),
                'region': region_code,
                'total_videos_analyzed': len(videos),
                'total_keywords_found': len(keyword_stats),
                'trending_keywords': trend_results[:50],
                'related_keywords': related_keywords,
                'temporal_trends': temporal_trends,
                'keyword_statistics': self._generate_keyword_statistics(keyword_stats)
            }
            
        except Exception as e:
            print(f"âŒ íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def _extract_keywords_from_videos(self, videos):
        """ì˜ìƒ ë©”íƒ€ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_stats = defaultdict(lambda: {
            'count': 0,
            'total_views': 0,
            'avg_views': 0,
            'videos': [],
            'categories': set(),
            'first_seen': None,
            'last_seen': None
        })
        
        print("ğŸ”¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        for i, video in enumerate(videos):
            try:
                # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
                title = video['snippet']['title']
                tags = video['snippet'].get('tags', [])
                description = video['snippet'].get('description', '')[:200]
                views = int(video['statistics'].get('viewCount', 0))
                category_id = video['snippet'].get('categoryId', 'Unknown')
                published_at = video['snippet']['publishedAt']
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                all_keywords = set()
                
                # 1. ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                title_keywords = self._extract_keywords_from_text(title)
                all_keywords.update(title_keywords)
                
                # 2. íƒœê·¸ ì •ì œ
                clean_tags = [self._clean_keyword(tag) for tag in tags[:10]]
                all_keywords.update([tag for tag in clean_tags if len(tag) >= 2])
                
                # 3. ì„¤ëª…ì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ
                hashtags = re.findall(r'#(\w+)', description)
                all_keywords.update([self._clean_keyword(tag) for tag in hashtags])
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                for keyword in all_keywords:
                    if len(keyword) >= 2:
                        stats = keyword_stats[keyword]
                        stats['count'] += 1
                        stats['total_views'] += views
                        stats['categories'].add(category_id)
                        
                        video_info = {
                            'title': title,
                            'views': views,
                            'channel': video['snippet']['channelTitle'],
                            'published_at': published_at,
                            'video_id': video['id']
                        }
                        stats['videos'].append(video_info)
                        
                        # ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
                        if stats['first_seen'] is None or published_at < stats['first_seen']:
                            stats['first_seen'] = published_at
                        if stats['last_seen'] is None or published_at > stats['last_seen']:
                            stats['last_seen'] = published_at
                
                if (i + 1) % 50 == 0:
                    print(f"   ì§„í–‰ë¥ : {i + 1}/{len(videos)}")
                    
            except Exception as e:
                print(f"âš ï¸ ì˜ìƒ ì²˜ë¦¬ ì˜¤ë¥˜ (ID: {video.get('id', 'Unknown')}): {e}")
                continue
        
        # í‰ê·  ì¡°íšŒìˆ˜ ê³„ì‚°
        for keyword, stats in keyword_stats.items():
            if stats['count'] > 0:
                stats['avg_views'] = stats['total_views'] // stats['count']
                stats['categories'] = list(stats['categories'])
        
        print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keyword_stats)}ê°œ ê³ ìœ  í‚¤ì›Œë“œ")
        return dict(keyword_stats)
    
    def _extract_keywords_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return []
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        if self.language == "ko" and self.okt:
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                nouns = self.okt.nouns(clean_text)
                keywords = [word for word in nouns if len(word) >= 2]
                
                # ë¶ˆìš©ì–´ ì œê±°
                stopwords = {'ê²ƒ', 'ìˆ˜', 'ë‚´', 'ê±°', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë”°ë¼', 'ëŒ€í•´', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ'}
                keywords = [word for word in keywords if word not in stopwords]
                
                return keywords[:10]  # ìƒìœ„ 10ê°œ
            except:
                pass
        
        # ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
        words = clean_text.lower().split()
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if len(word) > 2 and word not in stopwords]
        
        return keywords[:10]
    
    def _clean_keyword(self, keyword):
        """í‚¤ì›Œë“œ ì •ì œ"""
        # ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        keyword = re.sub(r'[^\w\sê°€-í£]', '', keyword.lower())
        return keyword.strip()
    
    def _calculate_trend_scores(self, keyword_stats):
        """íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°"""
        trend_scores = {}
        
        for keyword, stats in keyword_stats.items():
            # íŠ¸ë Œë“œ ì ìˆ˜ = (ë“±ì¥ ë¹ˆë„ Ã— í‰ê·  ì¡°íšŒìˆ˜) / 1000000
            # ì¶”ê°€ ê°€ì¤‘ì¹˜: ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„±, ìµœì‹ ì„±
            
            base_score = (stats['count'] * stats['avg_views']) / 1000000
            
            # ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
            category_bonus = min(len(stats['categories']) * 0.1, 0.5)
            
            # ìµœì‹ ì„± ë³´ë„ˆìŠ¤ (ìµœê·¼ 24ì‹œê°„ ë‚´ ë“±ì¥í•œ í‚¤ì›Œë“œ)
            recency_bonus = 0
            if stats['last_seen']:
                try:
                    last_seen_dt = datetime.fromisoformat(stats['last_seen'].replace('Z', '+00:00'))
                    hours_ago = (datetime.now(last_seen_dt.tzinfo) - last_seen_dt).total_seconds() / 3600
                    if hours_ago <= 24:
                        recency_bonus = 0.3
                except:
                    pass
            
            final_score = base_score * (1 + category_bonus + recency_bonus)
            
            trend_scores[keyword] = {
                'score': round(final_score, 2),
                'frequency': stats['count'],
                'avg_views': stats['avg_views'],
                'total_views': stats['total_views'],
                'category_count': len(stats['categories']),
                'categories': stats['categories'],
                'recency_hours': self._calculate_recency_hours(stats['last_seen']) if stats['last_seen'] else None,
                'sample_videos': sorted(stats['videos'], key=lambda x: x['views'], reverse=True)[:3]
            }
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_trends = sorted(trend_scores.items(), 
                              key=lambda x: x[1]['score'], 
                              reverse=True)
        
        return [{'keyword': k, **v} for k, v in sorted_trends]
    
    def _analyze_keyword_relationships(self, keyword_stats):
        """í‚¤ì›Œë“œ ê°„ ì—°ê´€ì„± ë¶„ì„"""
        print("ğŸ”— í‚¤ì›Œë“œ ì—°ê´€ì„± ë¶„ì„ ì¤‘...")
        
        keyword_cooccurrence = defaultdict(lambda: defaultdict(int))
        
        # ë™ì¼ ì˜ìƒì— ë“±ì¥í•œ í‚¤ì›Œë“œë“¤ì˜ ë™ì‹œ ë“±ì¥ ë¹ˆë„ ê³„ì‚°
        for keyword, stats in keyword_stats.items():
            for video in stats['videos']:
                video_title = video['title'].lower()
                
                # ì´ ì˜ìƒì— ë“±ì¥í•œ ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ ì°¾ê¸°
                for other_keyword in keyword_stats.keys():
                    if other_keyword != keyword and other_keyword.lower() in video_title:
                        keyword_cooccurrence[keyword][other_keyword] += 1
        
        # ê° í‚¤ì›Œë“œë³„ ìƒìœ„ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ
        related_keywords = {}
        for keyword in keyword_stats.keys():
            if keyword in keyword_cooccurrence:
                related = sorted(keyword_cooccurrence[keyword].items(), 
                               key=lambda x: x[1], reverse=True)[:5]
                
                related_keywords[keyword] = [
                    {
                        'keyword': rel_keyword,
                        'cooccurrence_count': count,
                        'relevance_score': round(count / keyword_stats[keyword]['count'], 3)
                    }
                    for rel_keyword, count in related
                ]
        
        return related_keywords
    
    def _analyze_temporal_trends(self, videos, keyword_stats):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        print("â° ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        
        # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ ë“±ì¥ ë¹ˆë„
        hourly_trends = defaultdict(lambda: defaultdict(int))
        daily_trends = defaultdict(lambda: defaultdict(int))
        
        for video in videos:
            try:
                published_at = video['snippet']['publishedAt']
                dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                
                hour = dt.hour
                day = dt.strftime('%A')
                title = video['snippet']['title']
                
                # ì´ ì˜ìƒì˜ í‚¤ì›Œë“œë“¤ ì°¾ê¸°
                video_keywords = []
                for keyword in keyword_stats.keys():
                    if keyword.lower() in title.lower():
                        video_keywords.append(keyword)
                        hourly_trends[hour][keyword] += 1
                        daily_trends[day][keyword] += 1
                        
            except Exception as e:
                continue
        
        # ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€/ìš”ì¼ ì°¾ê¸°
        peak_hours = {}
        peak_days = {}
        
        for hour, keywords in hourly_trends.items():
            total_activity = sum(keywords.values())
            peak_hours[hour] = total_activity
        
        for day, keywords in daily_trends.items():
            total_activity = sum(keywords.values())
            peak_days[day] = total_activity
        
        most_active_hour = max(peak_hours, key=peak_hours.get) if peak_hours else None
        most_active_day = max(peak_days, key=peak_days.get) if peak_days else None
        
        return {
            'hourly_distribution': dict(hourly_trends),
            'daily_distribution': dict(daily_trends),
            'peak_activity': {
                'hour': most_active_hour,
                'day': most_active_day,
                'hour_activity': peak_hours.get(most_active_hour, 0) if most_active_hour else 0,
                'day_activity': peak_days.get(most_active_day, 0) if most_active_day else 0
            }
        }
    
    def _generate_keyword_statistics(self, keyword_stats):
        """í‚¤ì›Œë“œ í†µê³„ ìƒì„±"""
        if not keyword_stats:
            return {}
        
        # ê¸°ë³¸ í†µê³„
        total_keywords = len(keyword_stats)
        frequencies = [stats['count'] for stats in keyword_stats.values()]
        avg_frequencies = [stats['avg_views'] for stats in keyword_stats.values()]
        
        return {
            'total_unique_keywords': total_keywords,
            'frequency_stats': {
                'min': min(frequencies) if frequencies else 0,
                'max': max(frequencies) if frequencies else 0,
                'avg': round(sum(frequencies) / len(frequencies), 2) if frequencies else 0
            },
            'view_stats': {
                'min_avg_views': min(avg_frequencies) if avg_frequencies else 0,
                'max_avg_views': max(avg_frequencies) if avg_frequencies else 0,
                'overall_avg_views': round(sum(avg_frequencies) / len(avg_frequencies), 2) if avg_frequencies else 0
            },
            'distribution': {
                'high_frequency_keywords': len([f for f in frequencies if f >= 10]),
                'medium_frequency_keywords': len([f for f in frequencies if 5 <= f < 10]),
                'low_frequency_keywords': len([f for f in frequencies if f < 5])
            }
        }
    
    def _calculate_recency_hours(self, timestamp):
        """ìµœì‹ ì„± ê³„ì‚° (ì‹œê°„ ë‹¨ìœ„)"""
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            hours_ago = (datetime.now(dt.tzinfo) - dt).total_seconds() / 3600
            return round(hours_ago, 1)
        except:
            return None
    
    def compare_trends(self, region1='KR', region2='US', max_results=100):
        """
        ì§€ì—­ë³„ íŠ¸ë Œë“œ ë¹„êµ
        
        Args:
            region1 (str): ì²« ë²ˆì§¸ ì§€ì—­ ì½”ë“œ
            region2 (str): ë‘ ë²ˆì§¸ ì§€ì—­ ì½”ë“œ
            max_results (int): ë¶„ì„í•  ì˜ìƒ ìˆ˜
            
        Returns:
            dict: ì§€ì—­ë³„ íŠ¸ë Œë“œ ë¹„êµ ê²°ê³¼
        """
        print(f"ğŸŒ ì§€ì—­ë³„ íŠ¸ë Œë“œ ë¹„êµ: {region1} vs {region2}")
        
        try:
            # ê° ì§€ì—­ì˜ íŠ¸ë Œë“œ ë¶„ì„
            trends1 = self.analyze_trending_keywords(region1, max_results)
            trends2 = self.analyze_trending_keywords(region2, max_results)
            
            if 'error' in trends1 or 'error' in trends2:
                return {'error': 'ì§€ì—­ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨'}
            
            # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°
            keywords1 = set(item['keyword'] for item in trends1['trending_keywords'])
            keywords2 = set(item['keyword'] for item in trends2['trending_keywords'])
            
            common_keywords = keywords1 & keywords2
            unique_to_region1 = keywords1 - keywords2
            unique_to_region2 = keywords2 - keywords1
            
            # ì§€ì—­ë³„ íŠ¹ì„± ë¶„ì„
            comparison = {
                'analysis_time': datetime.now().isoformat(),
                'regions_compared': [region1, region2],
                'region1_stats': {
                    'total_keywords': len(keywords1),
                    'unique_keywords': len(unique_to_region1),
                    'top_keywords': [item['keyword'] for item in trends1['trending_keywords'][:10]]
                },
                'region2_stats': {
                    'total_keywords': len(keywords2),
                    'unique_keywords': len(unique_to_region2),
                    'top_keywords': [item['keyword'] for item in trends2['trending_keywords'][:10]]
                },
                'comparison': {
                    'common_keywords_count': len(common_keywords),
                    'common_keywords': list(common_keywords)[:20],
                    'unique_to_region1': list(unique_to_region1)[:10],
                    'unique_to_region2': list(unique_to_region2)[:10],
                    'similarity_score': round(len(common_keywords) / len(keywords1 | keywords2) * 100, 2) if (keywords1 | keywords2) else 0
                }
            }
            
            return comparison
            
        except Exception as e:
            print(f"âŒ ì§€ì—­ë³„ íŠ¸ë Œë“œ ë¹„êµ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def detect_emerging_trends(self, region_code='KR', hours_threshold=6):
        """
        ì‹ í¥ íŠ¸ë Œë“œ ê°ì§€
        
        Args:
            region_code (str): ì§€ì—­ ì½”ë“œ
            hours_threshold (int): ì‹ í¥ íŠ¸ë Œë“œ ê¸°ì¤€ ì‹œê°„ (ì‹œê°„)
            
        Returns:
            dict: ì‹ í¥ íŠ¸ë Œë“œ ì •ë³´
        """
        print(f"ğŸš€ ì‹ í¥ íŠ¸ë Œë“œ ê°ì§€ ì¤‘... (ìµœê·¼ {hours_threshold}ì‹œê°„)")
        
        try:
            # íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„
            trends = self.analyze_trending_keywords(region_code)
            
            if 'error' in trends:
                return trends
            
            # ìµœê·¼ ë“±ì¥í•œ í‚¤ì›Œë“œë“¤ í•„í„°ë§
            emerging_keywords = []
            
            for item in trends['trending_keywords']:
                recency_hours = item.get('recency_hours')
                if recency_hours and recency_hours <= hours_threshold:
                    emerging_keywords.append({
                        'keyword': item['keyword'],
                        'trend_score': item['score'],
                        'frequency': item['frequency'],
                        'hours_since_first_seen': recency_hours,
                        'sample_videos': item['sample_videos']
                    })
            
            # ê¸‰ìƒìŠ¹ ì ìˆ˜ ê³„ì‚° (ë¹ˆë„ / ì‹œê°„)
            for keyword in emerging_keywords:
                keyword['velocity_score'] = round(
                    keyword['frequency'] / max(keyword['hours_since_first_seen'], 0.1), 2
                )
            
            # ê¸‰ìƒìŠ¹ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            emerging_keywords.sort(key=lambda x: x['velocity_score'], reverse=True)
            
            return {
                'analysis_time': datetime.now().isoformat(),
                'region': region_code,
                'hours_threshold': hours_threshold,
                'emerging_trends_count': len(emerging_keywords),
                'emerging_keywords': emerging_keywords[:20],
                'detection_summary': {
                    'total_analyzed': len(trends['trending_keywords']),
                    'emerging_detected': len(emerging_keywords),
                    'emergence_rate': round(len(emerging_keywords) / len(trends['trending_keywords']) * 100, 2) if trends['trending_keywords'] else 0
                }
            }
            
        except Exception as e:
            print(f"âŒ ì‹ í¥ íŠ¸ë Œë“œ ê°ì§€ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}